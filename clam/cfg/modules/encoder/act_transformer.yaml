# @package encoder

name: act_transformer
n_encoder_layers: 6
n_layers: 6
dim_model: 512
dim_feedforward: 2048
n_heads: 8
dropout: 0.1
pre_norm: False
feedforward_activation: gelu
seq_len: ${data.seq_len}

pos_enc: learned